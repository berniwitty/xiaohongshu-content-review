"""
å°çº¢ä¹¦å†…å®¹å®¡æ ¸åŠ©æ‰‹ Demo
åŸºäº Qwen2-VL + LoRA SFT å¾®è°ƒ
"""

import gradio as gr
from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
from qwen_vl_utils import process_vision_info
import torch

# ============ é…ç½® ============
MODEL_PATH = "D:/2025 Content Review Assistant/LLaMA-Factory/models/qwen2vl-content-review-sft-merged" # æ”¾ç½®æ¨¡å‹çš„è·¯å¾„

SYSTEM_PROMPT = """ä½ æ˜¯å°çº¢ä¹¦å†…å®¹å®¡æ ¸åŠ©æ‰‹ï¼Œè´Ÿè´£åˆ¤æ–­ç”¨æˆ·å‘å¸ƒçš„å†…å®¹æ˜¯å¦ç¬¦åˆå¹³å°è§„èŒƒã€‚

è¯·æ ¹æ®ä»¥ä¸‹è§„åˆ™è¿›è¡Œå®¡æ ¸ï¼š
1. ç¦æ­¢è™šå‡å®£ä¼ å’Œå¤¸å¤§åŠŸæ•ˆ
2. ç¦æ­¢å¼•æµåˆ°ç§åŸŸï¼ˆå¾®ä¿¡ã€QQç­‰ï¼‰
3. ç¦æ­¢è¿è§„åŒ»ç–—å¥åº·å£°æ˜
4. ç¦æ­¢ä½ä¿—ã€æš´åŠ›ã€è¿æ³•å†…å®¹
5. ç¦æ­¢æŠ„è¢­å’Œä¾µæƒå†…å®¹
6. ç¦æ­¢è¯±å¯¼äº’åŠ¨ï¼ˆæ±‚èµã€æ±‚å…³æ³¨ï¼‰
7. ç¦æ­¢æ¬ºè¯ˆå’Œéæ³•æœåŠ¡

å®¡æ ¸ç»“æœåˆ†ä¸ºï¼šé€šè¿‡(pass)ã€éœ€è¦ä¿®æ”¹(needs_edit)ã€ç§»é™¤(remove)ã€å‡çº§äººå·¥å®¡æ ¸(escalate)"""

# ============ åŠ è½½æ¨¡å‹ ============
print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
model = Qwen2VLForConditionalGeneration.from_pretrained(
    MODEL_PATH,
    torch_dtype=torch.float16,
    device_map="auto"
)
processor = AutoProcessor.from_pretrained(MODEL_PATH)
print("æ¨¡å‹åŠ è½½å®Œæˆï¼")


def review_content(text: str, image=None) -> str:
    """å®¡æ ¸å†…å®¹"""
    if not text.strip():
        return "è¯·è¾“å…¥å¾…å®¡æ ¸çš„æ–‡æ¡ˆå†…å®¹"
    
    # æ„å»ºæ¶ˆæ¯
    user_content = f"è¯·å®¡æ ¸è¿™æ®µæ–‡æ¡ˆï¼šã€Œ{text}ã€"
    
    if image is not None:
        messages = [
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": [
                {"type": "image", "image": image},
                {"type": "text", "text": user_content}
            ]}
        ]
    else:
        messages = [
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": user_content}
        ]
    
    # å¤„ç†è¾“å…¥
    text_input = processor.apply_chat_template(
        messages, 
        tokenize=False, 
        add_generation_prompt=True
    )
    
    if image is not None:
        image_inputs, video_inputs = process_vision_info(messages)
        inputs = processor(
            text=[text_input],
            images=image_inputs,
            videos=video_inputs,
            padding=True,
            return_tensors="pt"
        ).to(model.device)
    else:
        inputs = processor(
            text=[text_input],
            padding=True,
            return_tensors="pt"
        ).to(model.device)
    
    # ç”Ÿæˆ
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=512,
            do_sample=False,
            pad_token_id=processor.tokenizer.pad_token_id
        )
    
    # è§£ç 
    generated_ids = outputs[:, inputs.input_ids.shape[1]:]
    response = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
    
    return response


# ============ Gradio ç•Œé¢ ============
with gr.Blocks(
    title="å°çº¢ä¹¦å†…å®¹å®¡æ ¸åŠ©æ‰‹",
    theme=gr.themes.Soft()
) as demo:
    
    gr.Markdown("""
    # ğŸ” å°çº¢ä¹¦å†…å®¹å®¡æ ¸åŠ©æ‰‹
    
    åŸºäº **Qwen2-VL-2B + LoRA SFT** å¾®è°ƒçš„å¤šæ¨¡æ€å†…å®¹å®¡æ ¸ç³»ç»Ÿ
    
    **å®¡æ ¸ç±»åˆ«ï¼š** é€šè¿‡ âœ… | éœ€è¦ä¿®æ”¹ âœï¸ | è¿è§„åˆ é™¤ âŒ | äººå·¥å¤æ ¸ ğŸ‘¤
    """)
    
    with gr.Row():
        with gr.Column(scale=1):
            text_input = gr.Textbox(
                label="ğŸ“ å¾…å®¡æ ¸æ–‡æ¡ˆ",
                placeholder="è¾“å…¥å°çº¢ä¹¦é£æ ¼çš„æ–‡æ¡ˆï¼Œä¾‹å¦‚ï¼šåˆ†äº«ä»Šå¤©çš„ç©¿æ­lookï½",
                lines=4
            )
            image_input = gr.Image(
                label="ğŸ–¼ï¸ é…å›¾ï¼ˆå¯é€‰ï¼‰", 
                type="pil"
            )
            submit_btn = gr.Button("ğŸš€ å¼€å§‹å®¡æ ¸", variant="primary", size="lg")
        
        with gr.Column(scale=1):
            output = gr.Textbox(
                label="ğŸ“‹ å®¡æ ¸ç»“æœ",
                lines=12,
                show_copy_button=True
            )
    
    # ç¤ºä¾‹
    gr.Markdown("### ğŸ’¡ æµ‹è¯•æ ·ä¾‹")
    gr.Examples(
        examples=[
            ["åˆ†äº«ä»Šå¤©åšçš„åˆé¤ï¼Œç•ªèŒ„ç‚’è›‹ï¼Œç®€å•åˆå¥½åƒï½", None],
            ["è¿™æ¬¾ç¾ç™½éœœç”¨äº†ä¸€å‘¨ï¼Œçš®è‚¤ç™½äº†ä¸‰ä¸ªè‰²å·ï¼æ•ˆæœå¤ªæƒŠè‰³äº†", None],
            ["ç§æˆ‘é¢†å–å†…éƒ¨ä¼˜æƒ åˆ¸ï¼Œæ¯”å®˜æ–¹ä¾¿å®œ50%ï¼ä»…é™å‰100å", None],
            ["å§å¦¹ä»¬è¿™ä¸ªå‡è‚¥è¯çœŸçš„æœ‰ç”¨ï¼ä¸€ä¸ªæœˆç˜¦äº†20æ–¤ä¸åå¼¹", None],
            ["å›è´­äº†æ— æ•°æ¬¡çš„é¢è†œï¼Œç”¨ä¹ æƒ¯äº†ç¦»ä¸å¼€", None],
            ["ç‚¹èµè¿‡1000å°±æŠ½å¥–ï¼Œå¸®å¸®å¿™", None],
        ],
        inputs=[text_input, image_input],
        label="ç‚¹å‡»ä¸‹æ–¹ç¤ºä¾‹å¿«é€Ÿæµ‹è¯•"
    )
    
    submit_btn.click(
        fn=review_content, 
        inputs=[text_input, image_input], 
        outputs=output
    )
    
    gr.Markdown("""
    ---
    **é¡¹ç›®ä¿¡æ¯ï¼š** åŸºäº LLaMA-Factory æ¡†æ¶ | è®­ç»ƒæ•°æ® 1000+ æ¡ | RTX 4060 è®­ç»ƒçº¦ 12 åˆ†é’Ÿ
    """)


if __name__ == "__main__":
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=True  # ç”Ÿæˆå…¬ç½‘é“¾æ¥
    )
